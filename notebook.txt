有监督学习：
    sklearn 一般模型 都存在的方法 get_params() 获取模型参数的值，set_params() 设置模型参数的值

    有监督学习是对已知输入输出的数据，通过训练找到其规律，依据该模型对未知输出的输入预计其输出
    对于连续型的数据用 线性回归，
    对于离散行的数据用 分类
    线性回归
        h(x) = X*θ.T  其中X是 m*n的向量，θ向量是每个x的参数，X和θ的转置相乘得到对应的线性方程组 (h(x) = x0*θ0 + x1*θ1 + x2*θ2 + ... + xn*θn)
        线性回归的方程的系数θ求值 目前最主要的就是通过梯度下降和最小二乘法类确定其最合理的值
        最小二乘法：通过确定损失函数的最小值，找到theta(θ)向量的最优解，损失函数的构建为 数据集中每一个样例的估计值和实际值的平方差并求取平均
        sklearn.linear_model.LinearRegression() 参数:fit_intercept 是否计算该模型的截距，默认为True
                                                     normalize 是否归一标准化，根据fit_intercept设置, 默认为false
                                                     copy_X 如果为真，则复制X;否则，它可能被覆盖。默认为True
                                                     n_jobs 使用计算机cpu的核数
                                                线性回归对象属性：coef_模型的系数，intercept_模型的截距

    Lasso
        sklearn.linear_model.lasso() 参数：alpha 正则化系数，
                                            fit_intercept 是否计算该模型的截距，默认为True
                                            normalize 是否归一标准化，根据fit_intercept设置, 默认为false
                                            copy_X 如果为真，则复制X;否则，它可能被覆盖。默认为True
                                            max_iter 最大的迭代次数，对于sparse_cg和lsqr而言，默认次数取决于scipy.sparse.linalg，对于sag而言，则默认为1000次。
                                            warm_start：一个布尔值。如果为True，那么使用前一次训练结果继续训练，否则从头开始训练。
                                            positive：一个布尔值。如果为True，那么强制要求权重向量的分量都为整数。
                                            selection：一个字符串，可以选择‘cyclic’或者‘random’。它指定了当每轮迭代的时候，选择权重向量的哪个分量来更新。
                                                ‘ramdom’：更新的时候，随机选择权重向量的一个分量来更新。
                                                ‘cyclic’：更新的时候，从前向后一次选择权重向量的一个分量来更新。
                                            tol：精度
                                            random_state：sag的伪随机种子
    Ridge
        sklearn.linear_model.Ridge() 参数：alpha 正则化系数，
                                            fit_intercept 是否计算该模型的截距，默认为True
                                            normalize 是否归一标准化，根据fit_intercept设置, 默认为false
                                            copy_X 如果为真，则复制X;否则，它可能被覆盖。默认为True
                                            max_iter 最大的迭代次数，对于sparse_cg和lsqr而言，默认次数取决于scipy.sparse.linalg，对于sag而言，则默认为1000次。
                                            solver：在计算过程中选择的解决器
                                                auto：自动选择
                                                svd：奇异值分解法，比cholesky更适合计算奇异矩阵
                                                cholesky：使用标准的scipy.linalg.solve方法
                                                sparse_cg：共轭梯度法，scipy.sparse.linalg.cg,适合大数据的计算
                                                lsqr：最小二乘法，scipy.sparse.linalg.lsqr
                                                sag：随机平均梯度下降法，在大数据下表现良好。
                                                注：后四个方法都支持稀疏和密集数据，而sag仅在fit_intercept为True时支持密集数据。
                                            tol：精度
                                            random_state：sag的伪随机种子
        岭回归，当线性回归的当不健康的数据是的最下二乘法出现极大的扰动，噪声比较明显时，在最小二乘法后加上一个正则项
        因为正则项系数是未知的，可以通过交叉验证找到正则项alpha的值  model3_cv = GridSearchCV(model3,param_grid={'alpha': alpha}, cv=5)
    多项式回归
        线性回归只能解决一部分回归的问题，如果数据的形式是非线性的那么 线性回归就无法满足条件
        其中 LinearRegression() 根据情况可以换成Ridge 或者 lasso 回归
        其中 degree 是最重要的参数，介数过高会导致过拟合，过低会导致欠拟合import sklearn.preprocessing as pl
        model = Pipeline([('Ploy', pl.PolynomialFeatures(degree=i)),
                      ('std_scaler', pl.StandardScaler()),
                      ('Linear', sl.LinearRegression())])
    logistic回归（逻辑回归）
        sklearn.linear_model.LogisticRegression(C=1.0, solver='', class_weight)
        参数：
            penalty：惩罚项，str类型，可选参数为l1和l2，默认为l2
            c：正则化系数λ的倒数，float类型，默认为1.0。必须是正浮点型数。像SVM一样，越小的数值表示越强的正则化
            fit_intercept：是否存在截距或偏差，bool类型，默认为True。
            class_weight：用于标示分类模型中各种类型的权重
            random_state：随机数种子，int类型，可选参数，默认为无，仅在正则化优化算法为sag,liblinear时有用
            solver：优化算法选择参数，只有五个可选参数，即newton-cg,lbfgs,liblinear,sag,saga。默认为liblinear。solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：
                liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。
                lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
                newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
                sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。
                saga：线性收敛的随机优化算法的的变重。，
        逻辑回归主要解决多分类问题，1/1+e^-x.
        逻辑回归的成本函数 = 预测值和实际值之间的误差，
        成本函数的参数也是通过偏微分，或者 梯度下降求得 使得成本函数最小的参数值
        多元分类：逻辑回归对于多元分类的处理方法是将多分类假设为二分类问题
        假设：分类 A, B, C, D，现在需评估 F样本属于哪个分类
        实际分类  假定二分类1 分类概率1 假定二分类2 分类概率2  假定二分类3 分类概率3  假定二分类4 分类概率4
        A           1        0.7->A     0                     0                   0
        B           0                   1         0.3         0                   0
        C           0                   0                     1         0.6       0
        D           0                   0                     0                   1         0.8
    朴素贝叶斯
        p(A|B)P(B) = P(B|A)P(A)
        现在有四个分类A,B,C,D
        给出一组数据 X = [x1, x2, x3, x4]
        判断X属于A,B,C,D分类中的哪一个
        比较：P(A|X); P(B|X); P(C|X); P(D|X)的概率大小
        P(A|X) 正比 P(A)P(x1|A)P(x2|A)P(x3|A)P(x4|A)
    决策树（优先选择熵减最大特征）
        决策树最主要的是怎么对一个数据做划分，按什么区做划分, 选取的特征属性是可数的离散的直接可用，若特征是连续的则划分区间在用，（类似于序号不可能作为特征分类对的）
        用熵去评价一个这棵决策树分类，如果某一个特征比较混乱，是要对这一特征做处理。选择特征是看用信息增益或信息增益率
        自动聚合
        随机森林
        正向激励

        决策树选择熵减最大的特征进行切分，
        处理特征数多或者特征关联性比较强的时候出现的错误几率比较多
        决策数容易出现过拟合，可以 通过剪支或者随机森林解决这一问题

    支持向量机的分类
        svm的目标函数就是找到，所有点到所有的符合实际情况的分割线的最小值集合中的最大值，其首要目前是找到支持向量
        对于线性可分的可以用svm线性分类或者多项式分类，
        线性不可分的可以用，svm径向基分类(高斯函数)，核函数的svm需要的三个重要参数，1，C是表示每个类别间距离，C越大间距越小，c越小间距越大（但是要在合理的精度范围之内）
            gamma参数是高斯函数的宽度，对于不均衡要本很重要的处理参数就是权值，权值并不是越大越好，这三个参数的合理最优解可以通过交叉验证获取
        数据量较小或者适中时可以用支持向量机，如果数据量较大可以用logistic，支持向量机的分类效果较好
        基于SVM的线性分类
        基于SVM的多项式分类
        基于SVM的径向基分类
    聚类
        k-means（必须已知样本的分类数）:
            sklearn.cluster.Kmeans(n_clusters=4, init='k-means++', n_init=10)
            kmeans对位置分类的样本进行分类，主要原理有一下几步
                1. 将初始样本随机分配中心点
                2. 计算所有样本到各个中心点的距离
                3. 各个样本点根据距离划分到最最近的中心点
                4. 计算均值求出新的中心点
                5. 如果新的中心点和原中心点一直或者相近则结束算法 否则执行2-4步骤


            参数：
                n_clusters 设置分类数，
                init：'k-means++' 设置初始中心的选择算法
                n_init: 设置初始中心周围的最少样本数，如果样本数少于该值则重新选择样本点
            初始值对聚类中心有什么影响？
                当出现异常值的时候对聚类中心会向异常值方向偏移

            初始值得选择方式有哪些？
                1.可以随机选择
                2.选择之后计算其方差，如果存在两个或多个方差值较小并且其聚类中心较近，则可以确定这几个聚类中心选择不合理 （二分k-means）
                3.选择一个聚类中心点，计算聚类中心到其他样本点的距离，选择较大的作为第二个聚类中心点，依次类推一直选择到样本给定的聚类值为止

            查看模型分类中心点：
                model.cluster_centers_
        均值漂移算法 （对未知具体分类数的样本）：
            sklearn.cluster.MeanShift()
            原理：假设样本服从某一概率函数密度曲线，将样本数据跟据不同的概率密度函数进行匹配。最后匹配出与样本符合的改路密度分布，
            01.首先创建样本评估器
            02.建立模型

        基于凝聚层次算法的聚类
        基于DBSCAN(Density-Based Spatial Clustering of Applications with Noise，带噪声的基于密度的聚类方法)算法的聚类
    寻找最近邻（FNN）
    k紧邻（KNN）

    推荐引擎原理：
        pass
    nlp：
        pass


    交叉验证
    r2_score
    查准率，召回率，F1得分
    混淆矩阵
    性能报告
    验证曲线
    学习曲线
    样本均衡
    置信概率
    轮廓系数
        轮廓系数用来描述聚类效果的好坏，轮廓系数越接近1聚类效果越好，越接近-1聚类效果越不好
        一个簇内的样本到其他样本的平均距离为a
        一个簇内的每一个样本到器最邻近簇的样本的所有距离的平均值为b
        score = (b - a)/ max(a, b)




无监督学习：
    对未知的输出，根据其输入数据内部的一定规则进行分类，
    聚类
    成分分析